---
title: "HW3"
author: "政治系國際關係組 簡郁展 b06302267"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE}
library(tidyverse)
```

## 1. 英文資料前處理與斷詞

請記得把分析的檔案下載到你的工作目錄。資料取自 [Martijn Schoonvelde Dataverse](https://dataverse.harvard.edu/file.xhtml?persistentId=doi:10.7910/DVN/2PNZNU/I0I7GM&version=2.0)，COOL課程平台也有放資料檔(comb.corpus.Rdata，在03_code/data之下)。

```{r load}
# 使用load讀Rdata檔，會在讀取的同時，自動創建原始檔案的object，所以不用重新assign
load("data/comb.corpus.Rdata") 
glimpse(corpus)
corpus <- as_tibble(corpus)
```

**請使用stringr的函式，進行文字資料前處理。**

1. 篩選country欄位中，含有"DK"的觀察值
（請注意：原始資料中有"DK"與"DK "都要選取），並存成`dk_corpus`。

```{r dk_corpus}
unique(corpus$country)
dk_corpus <- corpus %>%
    filter(str_detect(string = country, 
                      pattern = "DK|DK "))

```

2. 請將所有存在於text欄位的數字都移除，同樣assign回`dk_corpus`。

```{r dk_corpus_rmnum}
dk_corpus <- dk_corpus %>%
    mutate(text = str_replace_all(
        string = text,
        pattern = "[0-9]+",
        replacement = ""
    ))

# verifying
#str_detect(string = dk_corpus$text, pattern = "[0-9]+")
```

3. 請使用tidytext的函式，進行斷詞後，只挑選date與斷詞後的欄位，另存成`dk_token`。

```{r dk_token}
dk_token <- dk_corpus %>%
    tidytext::unnest_tokens(input = text, output = word) %>%
    select(date, word)
```

4. 移除停用字（stopwords），並assign新的object為dk_token_clean。

```{r rm_stopwords}
library(tm)
stopword <- tibble(word = stopwords())
dk_token_clean <- dk_token %>%
    anti_join(stopword, by = "word")
```

5. 請將每個 token 字元數小於3的字移除（如if），並僅保留date與斷詞後的欄位，並assign回`dk_token_clean`。（提示：可使用str_length）

```{r dk_token_clean}
dk_token_clean <- dk_token_clean %>%
    filter(nchar(dk_token_clean$word) >= 3)

# dk_token_clean2 <- dk_token_clean %>%
    # filter(str_length(string = word) >= 3)
```

6. 請以日期分群，計算每個字在當日總字數所占的百分比。並將每日重複出現的字，化約成一筆觀察值（提示：可使用dplyr::distinct，並設定.keep_all參數為TRUE）。最後，將當日所占比重最高的字前10名列出，並由高到低排列（注意：這裡是指全部占比最高的10個字，所以**最後呈現的只有10個觀察值**）。

```{r dk_daily_top10}
dk_token_clean %>%
    group_by(date) %>%
    count(word) %>%
    mutate(perc = n/sum(n)*100) %>%
    distinct(.keep_all = TRUE) %>%
    ungroup() %>%
    top_n(n = 10, wt = perc) %>%
    arrange(desc(perc))

```

## 2. 中文資料

以下將使用有關公務人員報導的頭版新聞資料。

```{r data}
library(pdftools)

text_vec <- pdftools::pdf_text("data/udn_text.pdf")

udn_text <- tibble(id = seq(1, length(text_vec)),
        text = text_vec) %>% 
        filter(str_detect(text, "公務員")) %>% 
        mutate(text = str_squish(text))
```

1. 請將每篇新聞text欄位的開頭數字與空白消除，並請創造一個新欄位`date`，將新聞中的報導年月日抽取出來，例如2013-08-21。最後請依欄位順序id, date, text排列，並存成`udn_corpus`。

```{r udn_corpus}
udn_corpus <- udn_text %>%
    mutate(date = str_extract_all(string = text,
                                  pattern = "\\d{4}-\\d{2}-\\d{2}"),
           text = str_replace_all(string = text,
                                  pattern = "\\d+ |\\d{4}-\\d{2}-\\d{2} ",
                                  replacement = "")) %>%
    select(id, date, text)
```

2. 使用jiebaR進行中文斷詞，不用另訂使用者字典，但請保留同樣是data frame的格式（提示：使用purrr::map或是調整jiebaR::worker參數，以及使用tidyr::unnest），並僅挑選id, date以及斷詞後的三個欄位，存成`udn_token`。

```{r udn_token}
library(jiebaR)
library(ropencc)
seg_engine <- worker(bylines = TRUE)
udn_token <- udn_corpus %>%
    mutate(words = segment(code = text,
                           jiebar = seg_engine)) %>%
    select(id, date, words) %>%
    tidyr::unnest(words)

```

3. 製作停用字data frame。（這題送分！**但請不要更動這題的任何程式碼**）

```{r stopwords}
zh_stopwords <- tibble(stopword = stopwords::stopwords(language = "zh", source = "misc"))
        
```

4. 請將udn_token中的停用字移除後，存成`udn_token_clean`。

```{r udn_token_clean}
# 將停用字辭典與udn_token簡轉繁
trans <- converter(S2TWP)
zh_stopwords$stopword <- run_convert(trans, zh_stopwords$stopword)
udn_token$words <- run_convert(trans, udn_token$words)

# 移除停用字
udn_token_clean <- udn_token %>%
    anti_join(zh_stopwords, by = c("words" = "stopword"))
```

5. 計算各年每字出現的比例，請先將年份為NA的觀察值移除再計算。最後呈現時，將每年重複出現的字，化約成一筆觀察值（提示：可使用dplyr::distinct，並設定.keep_all參數為TRUE），將結果存成`udn_final`

```{r}
udn_final <- udn_token_clean %>%
    drop_na(date) %>%
    mutate(year = str_extract_all(string = date,
                                  pattern = "\\d{4}")) %>% # 從date中提取新增year colummn
    select(id, year, words) %>%
    group_by(year) %>% # 將字詞按年分群
    count(words) %>% # 計算各年每字的詞頻
    mutate(perc = n/sum(n)*100) %>% # 計算各年每字佔該年總字詞的比例
    ungroup() %>%
    distinct(.keep_all = TRUE)
```

6. 取出各年最高頻率的字，最後呈現的data frame應該只有12列（可以參考dplyr::slice的使用方法，或是dplyr::top_n，如果使用top_n，可能會有perc相同的情況，最後會變成14列），並請依照年份由近到遠排序（年份近的會在data frame的越前面幾列）。

```{r}
udn_final_top <- udn_final %>%
    group_by(year) %>%
    arrange(desc(perc)) %>%
    slice_head() %>%
    ungroup() %>%
    transform(year = as.numeric(year)) %>%
    arrange(desc(year))
udn_final_top
```

